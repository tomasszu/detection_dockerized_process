{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Annotate Video with Detections\n",
                "\n",
                "---\n",
                "\n",
                "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/develop/docs/notebooks/annotate-video-with-detections.ipynb)\n",
                "\n",
                "One of the most common requirements of computer vision applications is detecting objects in images and displaying bounding boxes around those objects. In this cookbook we'll walk through the steps on how to utilize the open source Roboflow ecosystem to accomplish this task on a video. Let's dive in! \n",
                "\n",
                "## Installing Dependencies \n",
                "\n",
                "In this cookbook we'll be utilizing the open source packages [Inference](https://inference.roboflow.com/) and [Supervision](https://supervision.roboflow.com/latest/) to accomplish our goals. Let's get those installed in our notebook with pip."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "!pip install -q inference \"supervision[assets]\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download a Video Asset\n",
                "\n",
                "First, let's download a video that we can detect objects in. Supervision comes with a great utility called Assets to help us hit the ground running. Wehn we run this script, the video is saved in our local directory and can be accessed with the variable `path_to_video`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from supervision.assets import download_assets, VideoAssets\n",
                "\n",
                "# Download a supervision video asset \n",
                "path_to_video = download_assets(VideoAssets.PEOPLE_WALKING)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Detecting Objects\n",
                "\n",
                "For this example, the objects in the video that we'd like to detect are people. In order to display bounding boxes around the people in the video, we first need a way to detect them. We'll be using the open source [Inference](https://github.com/roboflow/inference) package for this task. Inference allows us to quickly use thousands of models, including fine tuned models from [Roboflow Universe](https://universe.roboflow.com/), with a few lines of code.  We'll also utilize a few utilities for working with our video data from the [Supervision](https://github.com/roboflow/supervision) package."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import supervision as sv\n",
                "from supervision.assets import download_assets, VideoAssets\n",
                "from inference.models.utils import get_roboflow_model\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # Download the video asset from Supervision assets.\n",
                "    PATH_TO_VIDEO = download_assets(VideoAssets.PEOPLE_WALKING)\n",
                "\n",
                "    # Load a yolov8 nano model from roboflow.\n",
                "    model = get_roboflow_model(\"yolov8n-640\")\n",
                "\n",
                "    # Create a frame generator and video info object from supervision utilities.\n",
                "    frame_generator = sv.get_video_frames_generator(PATH_TO_VIDEO)\n",
                "\n",
                "    # Yield a single frame from the generator.\n",
                "    frame = next(frame_generator)\n",
                "\n",
                "    # Run inference on our frame.\n",
                "    result = model.infer(frame)[0]\n",
                "\n",
                "    # Parse result into detections data model.\n",
                "    detections = sv.Detections.from_inference(result)\n",
                "\n",
                "    # Display the detections data model.\n",
                "    print(detections)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First, we load our model using the method `get_roboflow_model()`. Notice how we pass in a `model_id`? We're using an [alias](https://inference.roboflow.com/reference_pages/model_aliases/) here. This is where we can pass in other models from Roboflow Universe like this [rock, paper, scissors](https://universe.roboflow.com/roboflow-58fyf/rock-paper-scissors-sxsw) model utilizing our roboflow api key. \n",
                "\n",
                "```\n",
                "model = get_roboflow_mode(\n",
                "    model_id=\"rock-paper-scissors-sxsw/11\", \n",
                "    api_key=\"roboflow_private_api_key\"\n",
                ")\n",
                "```\n",
                "\n",
                "If you don't have an api key, you can [create an free Roboflow account](https://app.roboflow.com/login). This model wouldn't be much help with detecting people, but it's a nice exercise to see how our code becomes model agnostic!\n",
                "\n",
                "We then create a `frame_generator` object and yeild a single frame for inference using `next()`. We pass our frame to `model.infer()` to run inference, then pass that data into a little helpfer function called `sv.Detections.from_inference()` to parse it. Lastly we print our detections to show we are in fact detecting a few people in the frame! \n",
                "\n",
                "## Saving Bounding Boxes to the Video\n",
                "\n",
                "Now comes the fun part. Let's wrap up our code by utilizing `Annotators` and a `VideoSink` to draw bounding boxes and save the resulting video respectively. Take a peak at the final code example below. This might can take up to a minute to run, since we're processing a full video. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import supervision as sv\n",
                "from supervision.assets import download_assets, VideoAssets\n",
                "from inference.models.utils import get_roboflow_model\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # Download the video asset from Supervision assets.\n",
                "    PATH_TO_VIDEO = download_assets(VideoAssets.PEOPLE_WALKING)\n",
                "\n",
                "    # Load a yolov8 nano model from roboflow.\n",
                "    model = get_roboflow_model(\"yolov8n-640\")\n",
                "\n",
                "    # Initalize the bounding box frame annotator.\n",
                "    box_annotator = sv.BoundingBoxAnnotator()\n",
                "\n",
                "    # Create a frame generator  object from video path.\n",
                "    frame_generator = sv.get_video_frames_generator(PATH_TO_VIDEO)\n",
                "\n",
                "    # Create a video info object from video path.\n",
                "    video_info = sv.VideoInfo.from_video_path(PATH_TO_VIDEO)\n",
                "\n",
                "    # Use a VideoSink context manager for saving frames of a video.\n",
                "    with sv.VideoSink(target_path=\"output.mp4\", video_info=video_info) as sink:\n",
                "\n",
                "        # Iterate through frames yielded from the frame_generator.\n",
                "        for frame in frame_generator:\n",
                "\n",
                "            # Run inference on our frame.\n",
                "            result = model.infer(frame)[0]\n",
                "\n",
                "            # Parse the result into the detections data model.\n",
                "            detections = sv.Detections.from_inference(result)\n",
                "\n",
                "            # Apply bounding box to detections on a copy of the frame.\n",
                "            annotated_frame = box_annotator.annotate(\n",
                "                scene=frame.copy(), \n",
                "                detections=detections\n",
                "            )\n",
                "\n",
                "            # Write the annotated frame to the video sink.\n",
                "            sink.write_frame(frame=annotated_frame)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " Notice that we create a `box_annoator` variable by initalizing a [BoundingBoxAnnotator](https://supervision.roboflow.com/latest/annotators/#boundingboxannotator). We can change the color and thickness, but for simplicity we keep the defaults. There are a ton of easy to use [annotators](https://supervision.roboflow.com/latest/annotators/) available in the Supervision package other than a bounding box that are fun to play with. Next, we create a `video_info` variable to pass information about the video to our `VideoSink`. The `VideoSink` is a cool little context manager that allows us to `write_frames()` to a video ouput file. Prior to writing the frame, we annotate a copy of it utilizing `box_annotator.annotate()`. Let's take a look at the resulting video. It will be installed locally and is called `output.mp4`. When run, you will see bounding boxes around the detections. Pretty awesome! "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We only scratched the surface of all of the customizable Annotators and additional features that Supervision and Inference have to offer. Stay tuned for more cookbooks on how to take advantge of them in your computer vision applications. Happy building! "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
